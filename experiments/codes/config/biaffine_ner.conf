####################
# Model
####################

# Biaffine-NER
biaffine_ner_model = {
    # Model
    model_name = biaffine_ner_model
    dropout_rate = 0.2

    # Training
    loss_function = cross_entropy
    # focal_loss_gamma = 2.0 # for loss_function=focal_loss
    adam_eps = 1e-6
    adam_weight_decay = 1e-2
    max_grad_norm = 1.0
    max_patience = 10
}

####################
# Model x Pretrained Language Model
####################

# Biaffine-NER x BERT (base, uncased)
biaffine_ner_model_bertbaseuncased = ${biaffine_ner_model}{
    # Pretrained
    bert_pretrained_name_or_path = bert-base-uncased
    max_seg_len = 512

    # Training
    bert_learning_rate = 5e-5
    # bert_learning_rate = 2e-5
    task_learning_rate = 1e-4
}

# Biaffine-NER x BERT (large, uncased)
biaffine_ner_model_bertlargeuncased = ${biaffine_ner_model}{
    # Pretrained
    bert_pretrained_name_or_path = bert-large-uncased
    max_seg_len = 512

    # Training
    bert_learning_rate = 1e-5
    task_learning_rate = 1e-4
}

# Biaffine-NER x SpanBERT (base, uncased)
biaffine_ner_model_spanbertbaseuncased = ${biaffine_ner_model}{
    # Pretrained
    bert_pretrained_name_or_path = SpanBERT/spanbert-base-uncased
    max_seg_len = 512

    # Training
    bert_learning_rate = 2e-5
    task_learning_rate = 1e-4
}

# Biaffine-NER x SpanBERT (large, uncased)
biaffine_ner_model_spanbertlargeuncased = ${biaffine_ner_model}{
    # Pretrained
    bert_pretrained_name_or_path = SpanBERT/spanbert-large-uncased
    max_seg_len = 512

    # Training
    bert_learning_rate = 1e-5
    task_learning_rate = 3e-4
}

# Biaffine-NER x RoBERTa (base)
biaffine_ner_model_robertabase = ${biaffine_ner_model}{
    # Pretrained
    bert_pretrained_name_or_path = roberta-base
    max_seg_len = 512

    # Training
    bert_learning_rate = 2e-5
    task_learning_rate = 1e-4
}

# Biaffine-NER x RoBERTa (large)
biaffine_ner_model_robertalarge = ${biaffine_ner_model}{
    # Pretrained
    bert_pretrained_name_or_path = roberta-large
    max_seg_len = 512

    # Training
    bert_learning_rate = 1e-5
    task_learning_rate = 1e-4
}

# Biaffine-NER x SciBERT (uncased)
biaffine_ner_model_scibertuncased = ${biaffine_ner_model}{
    # Pretrained
    bert_pretrained_name_or_path = allenai/scibert_scivocab_uncased
    max_seg_len = 512

    # Training
    bert_learning_rate = 2e-5
    task_learning_rate = 1e-4
}

####################
# Model x Pretrained Language Model x Dataset
####################

# Biaffine-NER x SciBERT (uncased) x CDR
biaffine_ner_model_scibertuncased_cdr = ${biaffine_ner_model_scibertuncased}{
    # Dataset
    dataset_name = cdr
    allow_nested_entities = true

    # Training
    max_epoch = 30
    batch_size = 4
    gradient_accumulation_steps = 1
    warmup_ratio = 0.01
    n_steps_for_monitoring = 20
    n_steps_for_validation = -1
}

# ---

# Biaffine-NER x RoBERTa (large) x CoNLL 2003
biaffine_ner_model_robertalarge_conll2003 = ${biaffine_ner_model_robertalarge}{
    # Dataset
    dataset_name = conll2003
    allow_nested_entities = false

    # Training
    max_epoch = 30
    batch_size = 4
    gradient_accumulation_steps = 1
    warmup_ratio = 0.01
    n_steps_for_monitoring = 20
    n_steps_for_validation = -1
}

# ---

# Biaffine-NER x BERT (base; uncased) x Linked-DocRED
biaffine_ner_model_bertbaseuncased_linked_docred = ${biaffine_ner_model_bertbaseuncased}{
    # Dataset
    dataset_name = linked_docred
    allow_nested_entities = true

    # Training
    max_epoch = 30
    batch_size = 4
    gradient_accumulation_steps = 1
    warmup_ratio = 0.01
    n_steps_for_monitoring = 20
    n_steps_for_validation = -1
}

# ---

# Biaffine-NER x SciBERT (uncased) x CDR
biaffine_ner_model_scibertuncased_medmentions = ${biaffine_ner_model_scibertuncased}{
    # Dataset
    dataset_name = medmentions
    allow_nested_entities = true

    # Training
    max_epoch = 30
    batch_size = 4
    gradient_accumulation_steps = 1
    warmup_ratio = 0.01
    n_steps_for_monitoring = 20
    n_steps_for_validation = -1
}

