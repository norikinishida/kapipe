####################
# Model
####################

# LLM
llm = {
    # Model
    model_name = llm
    quantization_bits = 4

    # Model (generation method)
    # stop_list = ["\nHuman:", "\n'''\n", "\nQuestion:", "<|endoftext|>", "</s>"]
    stop_list = ["\nHuman:", "\n'''\n", "\nQuestion:", "<|endoftext|>", "</s>", "<eos>", "<|eot_id|>"]
    max_new_tokens = 512
    beam_size = 1
    do_sample = false
    num_return_sequences = 1
    clean_up_tokenization_spaces = false

    # Model (In-Context Learning)
    n_demonstrations = 2
    prompt_template_name_or_path = "ed_04"
    use_chat_prompt = true
}

####################
# Model x Pretrained Language Model
####################

# LLM x Llama2 (7B)
llm_llama2_7b = ${llm}{
    llm_name_or_path = meta-llama/Llama-2-7b-chat-hf
    max_seg_len = 4096
}

# LLM x Llama2 (13B)
llm_llama2_13b = ${llm}{
    llm_name_or_path = meta-llama/Llama-2-13b-chat-hf
    max_seg_len = 4096
}

# LLM x Llama3 (8B)
llm_llama3_8b = ${llm}{
    llm_name_or_path = meta-llama/Meta-Llama-3-8B-Instruct
    max_seg_len = 4096
}

####################
# Model x Pretrained Language Model x Dataset
####################

# LLM x Llama2 (7B) x CDR x ed_04
llm_llama2_7b_cdr_prompt04 = ${llm_llama2_7b}{
    # Dataset
    dataset_name = cdr

    # Prompt
    prompt_template_name_or_path = ed_04
    knowledge_base_name = MeSH
}

# LLM x Llama2 (13B) x CDR x ed_04
llm_llama2_13b_cdr_prompt04 = ${llm_llama2_13b}{
    # Dataset
    dataset_name = cdr

    # Prompt
    prompt_template_name_or_path = ed_04
    knowledge_base_name = MeSH
}

# LLM x Llama3 (8B) x CDR x ed_04
llm_llama3_8b_cdr_prompt04 = ${llm_llama3_8b}{
    # Dataset
    dataset_name = cdr

    # Prompt
    prompt_template_name_or_path = ed_04
    knowledge_base_name = MeSH
}
