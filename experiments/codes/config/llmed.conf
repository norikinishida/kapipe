####################
# Model (platform)
####################

# HuggingFace
hf = {
    # Model
    model_name = "hf"

    # Generation
    max_new_tokens = 512
    beam_size = 1
    do_sample = false
    num_return_sequences = 1
    # stop_list = ["\nHuman:", "\n'''\n", "\nQuestion:", "<|endoftext|>", "</s>"]
    stop_list = ["\nHuman:", "\n'''\n", "\nQuestion:", "<|endoftext|>", "</s>", "<eos>", "<|eot_id|>"]
    clean_up_tokenization_spaces = false

    # Prompt
    prompt_template_name_or_path = "ed_10_fewshot"
    n_demonstrations = 1
}

# OpenAI
openai = {
    # Model
    model_name = "openai"

    # Generation
    max_new_tokens = 512

    # Prompt
    prompt_template_name_or_path = "ed_10_fewshot"
    n_demonstrations = 1
}

####################
# Model x Pretrained Language Model
####################

# HuggingFace x Llama3 (8B)
hf_llama3_8b = ${hf}{
    # Model
    llm_name_or_path = meta-llama/Meta-Llama-3-8B-Instruct
    max_seg_len = 4096
    quantization_bits = 4
}

openai_gpt4omini = ${openai}{
    # Model
    openai_model_name = "gpt-4o-mini"
}

####################
# Model x Pretrained Language Model x Dataset
####################

# HuggingFace x Llama3 (8B) x CDR x ed_10_fewshot
hf_llama3_8b_cdr_prompt10fewshot = ${hf_llama3_8b}{
    # Dataset
    dataset_name = cdr

    # Prompt
    prompt_template_name_or_path = ed_10_fewshot
    knowledge_base_name = MeSH
}

# OpenAI x gpt-4o-mini x CDR x ed_10_fewshot
openai_gpt4omini_cdr_prompt10fewshot = ${openai_gpt4omini}{
    # Dataset
    dataset_name = cdr

    # Prompt
    prompt_template_name_or_path = ed_10_fewshot
    knowledge_base_name = MeSH
}

# OpenAI x gpt-4o-mini x Linked-DocRED x ed_10_fewshot
openai_gpt4omini_linked_docred_prompt10fewshot = ${openai_gpt4omini}{
    # Dataset
    dataset_name = linked_docred

    # Prompt
    prompt_template_name_or_path = ed_10_fewshot
    knowledge_base_name = Wikipedia
}
