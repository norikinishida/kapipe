####################
# Model
####################

# ATLOP
atlop_model = {
    # Model
    model_name = atlop_model
    bilinear_block_size = 64
    # dropout_rate = 0.3
    dropout_rate = 0.0
    token_embedding_method = independent # or overlap
    entity_pooling_method = logsumexp # or sum, mean, max
    use_localized_context_pooling = true

    # Training
    loss_function = adaptive_thresholding_loss # cross_entropy, focal_loss
    # focal_loss_gamma = 2.0 # for focal_loss
    adam_eps = 1e-6
    adam_weight_decay = 1e-2
    max_grad_norm = 1.0
    max_patience = 10
}

####################
# Model x Pretrained Language Model
####################

# ATLOP x BERT (base, cased)
atlop_model_bertbasecased = ${atlop_model}{
    # Pretrained
    bert_pretrained_name_or_path = bert-base-cased
    max_seg_len = 512

    # Training
    bert_learning_rate = 5e-5
    # bert_learning_rate = 2e-5
    task_learning_rate = 1e-4
}

# ATLOP x BERT (large, cased)
atlop_model_bertlargecased = ${atlop_model}{
    # Pretrained
    bert_pretrained_name_or_path = bert-large-cased
    max_seg_len = 512

    # Training
    bert_learning_rate = 1e-5
    task_learning_rate = 1e-4
}

# ATLOP x SpanBERT (base, cased)
atlop_model_spanbertbasecased = ${atlop_model}{
    # Pretrained
    bert_pretrained_name_or_path = SpanBERT/spanbert-base-cased
    max_seg_len = 512

    # Training
    bert_learning_rate = 2e-5
    task_learning_rate = 1e-4
}

# ATLOP x SpanBERT (large, cased)
atlop_model_spanbertlargecased = ${atlop_model}{
    # Pretrained
    bert_pretrained_name_or_path = SpanBERT/spanbert-large-cased
    max_seg_len = 512

    # Training
    bert_learning_rate = 1e-5
    task_learning_rate = 3e-4
}

# ATLOP x RoBERTa (base)
atlop_model_robertabase = ${atlop_model}{
    # Pretrained
    bert_pretrained_name_or_path = roberta-base
    max_seg_len = 512

    # Training
    bert_learning_rate = 2e-5
    task_learning_rate = 1e-4
}

# ATLOP x RoBERTa (large)
atlop_model_robertalarge = ${atlop_model}{
    # Pretrained
    bert_pretrained_name_or_path = roberta-large
    max_seg_len = 512

    # Training
    bert_learning_rate = 1e-5
    task_learning_rate = 1e-4
}

# ATLOP x SciBERT (cased)
atlop_model_scibertcased = ${atlop_model}{
    # Pretrained
    bert_pretrained_name_or_path = allenai/scibert_scivocab_cased
    max_seg_len = 512

    # Training
    bert_learning_rate = 2e-5
    task_learning_rate = 1e-4
}

####################
# Model x Pretrained Language Model x Dataset
####################

# ATLOP x SciBERT (cased) x CDR
atlop_model_scibertcased_cdr = ${atlop_model_scibertcased}{
    # Dataset
    dataset_name = cdr
    possible_head_entity_types = ["Chemical"]
    possible_tail_entity_types = ["Disease"]
    use_official_evaluation = false

    # Model
    top_k_labels = -1

    # Training
    max_epoch = 30
    batch_size = 4
    gradient_accumulation_steps = 1
    warmup_ratio = 0.06
    n_steps_for_monitoring = 20
    n_steps_for_validation = -1
}
atlop_model_scibertcased_cdr_overlap = ${atlop_model_scibertcased_cdr}{
    # Model
    token_embedding_method = overlap
    max_seg_len = 256
}

# ---

# ATLOP x SciBERT (cased) x GDA
atlop_model_scibertcased_gda = ${atlop_model_scibertcased}{
    # Dataset
    dataset_name = gda
    possible_head_entity_types = ["Gene"]
    possible_tail_entity_types = ["Disease"]
    use_official_evaluation = false

    # Model
    top_k_labels = -1

    # Training
    max_epoch = 10
    batch_size = 4
    gradient_accumulation_steps = 4
    warmup_ratio = 0.06
    n_steps_for_monitoring = 20
    n_steps_for_validation = 500
}

# ---

# ATLOP x BERT (base, cased) x DocRED
atlop_model_bertbasecased_docred = ${atlop_model_bertbasecased}{
    # Dataset
    dataset_name = docred
    possible_head_entity_types = null
    possible_tail_entity_types = null
    use_official_evaluation = true

    # Model
    top_k_labels = 4

    # Training
    max_epoch = 30
    batch_size = 4
    gradient_accumulation_steps = 1
    warmup_ratio = 0.06
    n_steps_for_monitoring = 20
    n_steps_for_validation = -1
}

# ---

# ATLOP x BERT (base, cased) x Re-DocRED
atlop_model_bertbasecased_redocred = ${atlop_model_bertbasecased}{
    # Dataset
    dataset_name = redocred
    possible_head_entity_types = null
    possible_tail_entity_types = null
    use_official_evaluation = true

    # Model
    top_k_labels = 4

    # Training
    max_epoch = 30
    batch_size = 4
    gradient_accumulation_steps = 1
    warmup_ratio = 0.06
    n_steps_for_monitoring = 20
    n_steps_for_validation = -1
}

# ---

# ATLOP x BERT (base; cased) x Linked-DocRED
atlop_model_bertbasecased_linked_docred = ${atlop_model_bertbasecased}{
    # Dataset
    dataset_name = linked_docred
    possible_head_entity_types = null
    possible_tail_entity_types = null
    use_official_evaluation = false

    # Model
    top_k_labels = 4

    # Training
    max_epoch = 30
    batch_size = 2
    gradient_accumulation_steps = 1
    warmup_ratio = 0.06
    n_steps_for_monitoring = 20
    n_steps_for_validation = -1
}

# ---

# ATLOP x SciBERT (cased) x MedMentions-DSREL
atlop_model_scibertcased_medmentions_dsrel = ${atlop_model_scibertcased}{
    # Dataset
    dataset_name = medmentions_dsrel
    possible_head_entity_types = null
    possible_tail_entity_types = null
    use_official_evaluation = false

    # Model
    top_k_labels = 4

    # Training
    max_epoch = 30
    batch_size = 4
    gradient_accumulation_steps = 1
    warmup_ratio = 0.06
    n_steps_for_monitoring = 20
    n_steps_for_validation = -1
}
atlop_model_scibertcased_medmentions_dsrel_overlap = ${atlop_model_scibertcased_medmentions_dsrel}{
    # Model
    token_embedding_method = overlap
    max_seg_len = 256
}
